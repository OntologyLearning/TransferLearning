<html>

<head>
<title>William J. Dally's Home Page</title>
</head>

<body>
<p><a href="images/people/currenter/Bill_Dally.jpg">
<img src="images/people/currenter/Bill_Dally.jpg" align=bottom width=300></a></p>

<h1>William J. Dally</h1>

<p><i>Last updated July 27, 2010</i> </p>

<p>
Bill Dally is the 
<a href="http://soe.stanford.edu/relations/endowed/bell.html">
   Willard R. and Inez Kerr Bell Professor</a> of
<a href="http://cs.stanford.edu">Computer Science</a> and
<a href="http://ee.stanford.edu">Electrical Engineering</a>
and former Chairman of the 
<a href="http://cs.stanford.edu">Computer Science Department</a>  
at 
<a href="http://www.stanford.edu">Stanford University</a>.
He is a member of the
<a href="http://csl.stanford.edu">Computer Systems Laboratory</a>, leads 
the 
<a href="http://cva.stanford.edu/">
Concurrent VLSI Architecture Group</a>, 
and teaches courses on Computer Architecture, 
Computer Design, and VLSI Design. 
He is a 
Member of the
<a href="http://www.nae.edu"> National Academy of Engineering </a>,
a Fellow of the 
<a href="http://www.amacad.org/"> American Academy of Arts & Sciences </a>, a Fellow of the
<a href="http://www.ieee.org">IEEE</a>, a Fellow of the 
<a href="http://www.acm.org">ACM</a>, received the 
<a href="http://www.acm.org/sigarch/wilkes.html">ACM Maurice Wilkes Award</a> in 2000, the 
<a href="http://www.computer.org/awards/"> IEEE Seymour Cray Award</a> in 2004, and the
<a href="http://awards.acm.org/eckert_mauchly/"> ACM Eckert Mauchly Award </a> in 2010.
He has an <a href="http://www.cs.ucla.edu/~palsberg/h-number.html"> h-index </a> of 60.
</p>
<p>
Before coming to Stanford, Bill was a Professor in the department of
<a href="http://www-eecs.mit.edu"> Electrical Engineering and Computer
Science </a> at <a href="http://www.mit.edu"> MIT </a>.

<h1>Current Projects</h1>
<dl>

<dt><a href="projects/elm">
ELM: The Efficient Low-Power Microprocessor</a></dt>
<dd>
We are developing a programmable architecture that is easily programmable in
a high-level language ("C") and at the same time has performance per unit 
power competitive with hard-wired logic, and 20-30x better than conventional
embedded RISC processors.
This power savings is achieved by using more efficient mechanisms for 
instruction supply, based on compiler managed instruction registers, and
data supply, using a deeper register hierarchy and indexable registers.
</dd>

<dt><a href="https://nocs.stanford.edu/">
Enabling Technology for On-Chip Networks</a> </dt>
<dd>
As CMPs and SoCs scale to include large numbers of cores and other
modules, the on-chip network or NoC that connects them becomes a critical
systems component.
We are developing enabling technology for on-chip networks including
network topologies, flow control mechanisms, and router organizations.
For example, our flattened butterfly topology offers both lower latency
and substantially reduced power compared to conventional on-chip mesh
or ring networks.
</dd>

<dt><a href="http://sequoia.stanford.edu">
Sequoia: Programming the Memory Hierarchy</a> </dt>
<dd>
Sequoia is a programming language that is designed to facilitate the
development of memory hierarchy aware parallel programs that remain
portable across modern machines with different memory hierarchy
configurations. Sequoia abstractly exposes hierarchical memory in the
programming model and provides language mechanisms to describe
communication vertically through the machine and to localize
computation to particular memory locations within the machine.

A complete Sequoia programming system has been implemented, including
a compiler and runtime systems for both Cell processors and
distributed memory clusters, that delivers efficient performance
running Sequoia programs on both of these platforms. An alpha version
of this programming system will soon be made public.  
</dd>

<dt><a href="projects/icn">
Scalable Network Fabrics</a></dt>

<dd> We are developing architectures and technologies to enable large,
scalable high-performance interconnection networks to be used in
parallel computers, network switches and routers, and high-performance
I/O systems.  Recent results include the development of a hierarchical
network topology that makes efficient use of a combination of
electrical and optical links, a locality-preserving randomized
oblivious routing algorithm, a method for scheduling constrained
crossbar switches, new speculative and reservation-based flow control
methods, and a method for computing the worst-case traffic pattern for
any oblivious routing function.
</dd>

</dl>

<h1> Recent Projects </h1>
<dl>

<dt><a href="http://merrimac.stanford.edu">
Streaming Supercomputer</a></dt>

<dd> We are developing a <i> streaming supercomputer </i> (SS) that is
scalable from a single-chip to thousands of chips that we estimate
will achieve an order of magnitude or more improvement in the performance per unit
cost on a wide range of demanding numerical computations compared to
conventional cluster-based supercomputers. The SS uses a combination of
stream processing with a high-performance network to access a globally
shared memory to achieve this goal.
</dd>

<dt><a href="projects/imagine/">
Imagine: A High-Performance Image and Signal Processor</a></dt>

<dd>Imagine is a programmable signal and image processor that provides
the performance and performance density of a special-purpose processor.
Imagine achieves a peak performance of 20GFLOPS (single-precision
floating point) and 40GOPS (16-bit fixed point) and sustains over
12GFLOPS and 20GOPS on key signal processing benchmarks.  Imagine
sustains a power efficiency of 3.7GFLOPS/W on these same benchmarks, a
factor of 20 better than the most efficient conventional signal
processors.
</dd>

<dt><a href="http://velox.stanford.edu/smart_memories/">
Smart Memories</a></dt>

<dd>We are investigating combined processor/memory architectures that
are best able to exploit 2009 semiconductor technologies.  We envision
these architectures being composed of 10s to 100s of processors and
memory banks on a single semiconductor chip.  Our research addresses
the design of the processors and memories, the architecture of the
interconnection network that ties them together, and mechanisms to
simplify programming of such machines.</dd>

<dt><a href="projects/hssp/">High-Speed Signalling</a></dt>
<dd>We are developing methods and circuits that stretch the
performance bounds of electrical signalling between chips, boards, and
cabinets in a digital system. A prototype 0.25um 4Gb/s CMOS
transceiver has been developed, dissipating only 130mW, amenable for
large scale integration.  Future chips include a a 20Gb/s 0.13um CMOS
transceiver.
</dd>

<dt><a href="projects/m-machine/">The M-Machine </a></dt>
<dd>Is an experimental parallel computer that demonstrated highly-efficient 
mechanisms for parallelism including two-level multithreading, efficient
network interfaces, fast communication and synchronization, and support
for efficient shared memory protocols. </dd>

<dt><a href="projects/router/">The Reliable Router </a></dt>
<dd>is a high-performance multicomputer router that demonstrates 
new technologies ranging from architecture to circuit design. At 
the architecture level the router uses a novel adaptive routing 
algorithm, a link-level retry protocol, and a unique token 
protocol. Together the two protocols greatly reduce the cost of 
providing reliable, exactly-once end-to-end communication. At 
the circuit level the router demonstrates the latest version of 
our simultaneous bidirectional pads and a new method for 
plesiochronous synchronization. </dd>
<dt><a href="projects/j-machine/">The J-Machine </a></dt>
<dd>is an experimental parallel computer, in operation since July 
1991, that demonstrates mechanisms that greatly reduce the 
overhead involved in inter-processor interaction. </dd>
</dl>

<h1>Publications</h1>

A complete list of publications and citations is available  
<a href=http://scholar.google.com/citations?user=YZHj-Y4AAAAJ&hl=en> here </a> from 
<a href=http://scholar.google.com> Google Scholar </a>.

Publications can be found at the
<a href="publications/">CVA group publications page </a>
</p>

Some selected publications are included below:

<ul>
<li>William Dally et al.
<a href = "publications/2004/spqueue.pdf">
"Stream Processors: Programmability with Efficiency"</a>
<i>ACM Queue</i>, March 2004, pp. 52-62.<p>

<li>William Dally et al.,
<a href = http://merrimac.stanford.edu/publications/sc03_merrimac.pdf>
"Merrimac: Supercomputing with Streams"</a>
<i>Supercomputing 2003</i></li><p>

<li>William J. Dally and Brian Towles,
<a href=http://books.elsevier.com/us/mk/us/subindex.asp?isbn=0122007514&country=United+States&community=mk&mscssid=2A8E5X4SJR2L8HQB1U9WLHR2PLPFF9AB>
<i>Principles and Practices of Interconnection Networks</i></a>
Morgan Kaufmann, 2004</li><p>

<li> William J. Dally and John W. Poulton, 
<a href="books/dig_sys_engr/">
<i> Digital Systems Engineering,  </i></a>
Cambridge University Press, 1998 </li><p>

<li> Fillo, Marco, <A
HREF="http://www.cs.utexas.edu/users/skeckler">Keckler, Stephen
W.</A>, <A HREF="http://csl.stanford.edu/~billd">Dally, William
J.</a>, <A
HREF="http://www.ai.mit.edu/people/npcarter/npcarter.html">Carter,
Nicholas P.</A>, Chang, Andrew, <a
href="http://www.ai.mit.edu/people/yev/yev.html">Gurevich,
Yevgeny</a>, and <A HREF="people/wslee">Lee,
Whay S.</a>, <a
href="publications/1997/ijpp.ps.Z"> "The
M-Machine Multicomputer" </a>, <I> International Journal of Parallel
Programming - Special Issue on Instruction-Level Parallel Processing
Part II </I>. Vol 25, No 3, 1997 pp 183-212. <p>

<li> <A HREF="http://csl.stanford.edu/~billd">Dally, William J.</a>,
Chang, Andrew., <a
href="http://www-csag.cs.uiuc.edu/individual/achien">Chien,
Andrew.</a>, Fiske, Stuart., Horwat, Waldemar., Keen, John., Lethin,
Richard., Noakes, Michael., Nuth, Peter., <A
HREF="http://www.mills.edu/ACAD_INFO/mcs_spertus.html">Spertus,
Ellen</a>., Wallach, Deborah., and <a
href="http://www.ee.gatech.edu/users/940/">Wills, D. Scott</a>. <br>
<a href="publications/1998/jm_retro.pdf"> "The
J-Machine" </a>. Retrospective in <I> 25 Years of the International
Symposia on Computer Architecture - Selected Papers</I>.  pp 54-58.
<p>

<li>
William J. Dally,
Virtual Channel Flow Control,
<i> IEEE Transactions on Parallel and Distributed Systems</i>
March, 1992, pp. 194-205.
</li>
<p>

</ul>
<h1>Companies</h1>

<p>Bill is Chief Scientist at <a href="http://www.nvidia.com"> NVIDIA </a>
where he was on leave during 2009 and 2010 as Chief Scientist and Senior Vice President of Research. </p>

<p>Bill has played a key role in founding several companies including:</p>
<dl>
<dt><a href="http://www.streamprocessors.com"> Stream Processors Inc.</a>(2004-2009)</dt>
<dd>to commercialize stream processors for embedded applications.</dd>

<dt>Velio Communications.  (CTO 1999-2003)</dt>
<dd>Velio pioneered high-speed I/O circuits and applied this
technology to integrated TDM and packet switching chips.  Velio's I/O
technology was acquired by <a href=http://www.rambus.com>Rambus</a>
and Velio itself was acquired by <a href=http://www.lsilogic.com>LSI
Logic</a>.</dd>

<dt><a href="http://www.avici.com">Avici Systems, Inc.</a>(1997-present)</dt>
<dd> Manufactures core Internet routers with industry-leading scalability and
reliability.</dd>
</dl>

<p>Bill has also worked with <a href=http://www.cray.com> Cray </a> since 1989
on the development of many of their supercomputers including the 
<a href="http://www.psc.edu/machines/cray/t3d/t3d.html"> T3D</a> and 
<a href="http://www.psc.edu/machines/cray/t3e/t3e.html"> T3E</a>.



<!--
<h1>Recent Talks, Etc...</h1>
<ul>
<li>
<a href="people/dally/ISSCC2005.pdf">Low-Power Architecture</a>, ISSCC 2005, February 7, 2005, San Francisco, California, USA.
</li>
<!-- <li>
<a href="./Stream_ICASSP_051904.pdf">
Stream Processing: ASIC Efficiency, Programmable in "C"</a> 
Plenary talk at ICASSP 2004, Montreal Canada, May 20, 2004.

<li>
<a href="people/dally/ARVLSI99.ppt">VLSI Architecture: Past, Present, and Future</a>
</ul>
</li>-->

<h1>Courses</h1>
<ul>

<li> <a href="classes/cs99s/"> CS 99S -- The Coming Revolution in Computer
Architecture (freshman seminar)</a>
<li> <a href="http://www.stanford.edu/class/ee108a">EE108A Digital Systems I</a></li>
<li><a href="http://www.stanford.edu/class/ee271">EE271 Introduction to VLSI Systems </a></li><li> <a href="classes/ee273/"> EE273 -- Digital Systems Engineering</a>
<li><a href="http://www.stanford.edu/class/ee282">EE282 Computer Architecture and Organization </a></li>
<li> <a href="classes/ee482a/"> EE482a--Advanced Computer Organization: Processor Microarchitecture</a>
<li> <a href="classes/ee482b/"> EE482b--Advanced Computer Organization: Interconnection Networks</a>


<li>6.823 Computer System Architecture </li>
<li>6.845 Concurrent VLSI Architecture </li>
<li><a href="http://www.ai.mit.edu/courses/6.915/6.915.html">6.915 Digital Systems Engineering </a></li>
</ul>

<h1><a href="people/">CVA People</a></h1>
</ul>

<hr>


<address>William J. Dally</address>
<address>&lt;dally "at" stanford "dot" edu&gt;</address>
<address>Stanford University </address>
<address>Computer Systems Laboratory </address>
<address>Gates Room 301 </address>
<address>Stanford, CA 94305 </address>
<address>(650) 725-8945 </address>
<address>FAX: (650) 725-6949 </address>
</body>

</html>
